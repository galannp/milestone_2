{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7eaaa1fc-6a2e-4739-ae3e-be90ffe237c9",
   "metadata": {},
   "source": [
    "# Project Milestone2 - Group Concatsanddogs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690106c9-c202-4b13-a41c-335544c35ecd",
   "metadata": {},
   "source": [
    "# The use of women's rights and gender equality rhetoric in the US"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6858764-e568-45ca-a871-5c5fa855b5b5",
   "metadata": {},
   "source": [
    "## Header"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8987c2ee-6b5d-496c-9450-163f1ab569a1",
   "metadata": {},
   "source": [
    "This notebook is organized followings the different steps used in our pipeline. We first create a list of keywords using [web scraping](#Webscraping) and a personal list of keywords. \n",
    "With this list of keywords we [select](#Dataset-selection-from-Quotebank-database) a subset of the Quotebank database. This subset will be our starting dataset for our project. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569a8fe1-2f1f-45ed-b256-75a7ddd90a72",
   "metadata": {},
   "source": [
    "## General librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef090096-3676-405b-a169-7efe712c5b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f294bf29-de89-4787-805b-0f98233329f1",
   "metadata": {},
   "source": [
    "## Webscraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef16436-c959-41b4-a35b-aacc530377ab",
   "metadata": {},
   "source": [
    "AMINA TO ADD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20430e1b-9f9c-445d-9e25-7b447c2395df",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62dbbea0-5c3f-46f1-a3a6-8f17c2db1913",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Younes\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import requests #http library\n",
    "import nltk #natural language processing library\n",
    "nltk.download('stopwords') #common english words to ignore \n",
    "from bs4 import BeautifulSoup #extraction from HTML and XML files\n",
    "from collections import Counter #dictionary subclass for counting hashable objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393d0fde-ee26-4582-abc4-445db2682a22",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a700f7a7-8666-4642-93c0-66ccfff1095b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ToDO: add input, output, requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99c65a6b-c004-4440-9eac-72de2da0c9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_urls_usnews(URL):\n",
    "    \n",
    "    response = requests.get(URL,headers=headers) #http request with a user-agent string to avoid blocking from server\n",
    "    soup = BeautifulSoup(response.text, 'html.parser') #parse the document with html format\n",
    "    latest = soup.find('div',{'class':\"LoadMoreWrapper__Container-zwyk5c-0 himujt\"}) #get all the elements within a specified tag\n",
    "    list_of_urls = []\n",
    "    for a in latest.find_all('a'):\n",
    "        list_of_urls.append(a['href'])\n",
    "    usnews_urls = list(set(list_of_urls))\n",
    "    \n",
    "    return usnews_urls\n",
    "\n",
    "def get_all_articles(usnews_urls):\n",
    "    all_articles = ''\n",
    "    for url in usnews_urls:\n",
    "        all_articles = all_articles +' '+article_from_url(url)\n",
    "    \n",
    "    return all_articles\n",
    "\n",
    "def article_from_url(url):\n",
    "    \n",
    "    response = requests.get(url,headers=headers) #http request with a user-agent string to avoid blocking from server\n",
    "    soup = BeautifulSoup(response.text, 'html.parser') #parse the document with html format\n",
    "    soup.find(type=\"application/ld+json\")\n",
    "    jsonArticle = json.loads(soup.find(type=\"application/ld+json\").string)\n",
    "    text=jsonArticle['articleBody']\n",
    "    clean_text = text.replace('.Copyright 2021 The&nbsp;Associated Press. All rights reserved. This material may not be published, broadcast, rewritten or redistributed.',' ').replace('Associated Press',' ').replace('quot',' ')\n",
    "    \n",
    "    return clean_text\n",
    "\n",
    "#Counting the frequency of n-grams in the text\n",
    "#Input: take the text as a single string of text\n",
    "#Output: \n",
    "#Requirement: Counter library\n",
    "def ngram_frequency(text):\n",
    "    #separate the text into words \n",
    "    allWords = nltk.tokenize.word_tokenize(text) \n",
    "    #gets rid on 1-gram and 2-gram\n",
    "    allLongWords = []\n",
    "    for word in allWords:\n",
    "        if len(word) > 2: \n",
    "            allLongWords.append(word)   \n",
    "    stopwords = nltk.corpus.stopwords.words('english') #list of words such as a, the, and etc..\n",
    "    allWordExceptStop =[]\n",
    "    #get rid of stopwords\n",
    "    for w in allLongWords:\n",
    "        if w.lower() not in stopwords:\n",
    "            allWordExceptStop.append(w)\n",
    "    bigrams = zip(allWordExceptStop, allWordExceptStop[1:])\n",
    "    bigramsFreq = nltk.FreqDist(bigrams) \n",
    "    return bigramsFreq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ea51ad-66bc-4c6f-b8d1-e96e25396f98",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f0b91d8-f3e5-46e3-96c3-30f35278d400",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of primary links containing articles of interest\n",
    "URL_TOPIC_LIST = ['https://www.usnews.com/topics/subjects/feminism',\n",
    "             'https://www.usnews.com/topics/subjects/gender',\n",
    "              'https://www.usnews.com/topics/subjects/gender_bias',\n",
    "             'https://www.usnews.com/topics/subjects/sexism']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9816feb4-0276-42f6-967f-2fb52ec511fd",
   "metadata": {},
   "source": [
    "### Retrieving of articles of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f138249b-1877-4aae-abb1-fc9d21465800",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'headers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2592/3477572829.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0murl_topic\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mURL_TOPIC_LIST\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;31m#Retrieve all urls for latest articles in the chosen topic pages\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0musnews_topic_urls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_urls_usnews\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl_topic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;31m#Retrieve all the articles contents for the latest articles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2592/1503388870.py\u001b[0m in \u001b[0;36mget_urls_usnews\u001b[1;34m(URL)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_urls_usnews\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mURL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mURL\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#http request with a user-agent string to avoid blocking from server\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'html.parser'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#parse the document with html format\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mlatest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'div'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'class'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m\"LoadMoreWrapper__Container-zwyk5c-0 himujt\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#get all the elements within a specified tag\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'headers' is not defined"
     ]
    }
   ],
   "source": [
    "all_articles = ''\n",
    "\n",
    "for url_topic in URL_TOPIC_LIST:\n",
    "    #Retrieve all urls for latest articles in the chosen topic pages\n",
    "    usnews_topic_urls = get_urls_usnews(url_topic)\n",
    "    \n",
    "    #Retrieve all the articles contents for the latest articles\n",
    "    all_articles_topic =  get_all_articles(usnews_topic_urls)\n",
    "    \n",
    "    #append articles to create one text\n",
    "    all_articles = all_articles +' '+all_articles_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337a0d5d-963b-449d-8947-f8e727be40d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export all the articles of interest in a single text file\n",
    "text_file = open(\"Articles-Contents.txt\", \"w\")\n",
    "text_file.write(all_articles)\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06aefa5-c5b5-4975-85f9-f87c573777aa",
   "metadata": {},
   "source": [
    "### Frequency computation for bigrams "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e66f7aa-98d1-4985-9018-19973e9c1f83",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Younes/nltk_data'\n    - 'C:\\\\ProgramData\\\\Anaconda3\\\\envs\\\\ada\\\\nltk_data'\n    - 'C:\\\\ProgramData\\\\Anaconda3\\\\envs\\\\ada\\\\share\\\\nltk_data'\n    - 'C:\\\\ProgramData\\\\Anaconda3\\\\envs\\\\ada\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Younes\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2592/392754498.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Couting bigram frequencies for all articles of interest\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0musNewsFEMbigramFreq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mngram_frequency\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_articles\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mMAX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m150\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2592/1503388870.py\u001b[0m in \u001b[0;36mngram_frequency\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mngram_frequency\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[1;31m#separate the text into words\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m     \u001b[0mallWords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m     \u001b[1;31m#gets rid on 1-gram and 2-gram\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[0mallLongWords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\ada\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    127\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m     \"\"\"\n\u001b[1;32m--> 129\u001b[1;33m     \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    130\u001b[0m     return [\n\u001b[0;32m    131\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\ada\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m     \"\"\"\n\u001b[1;32m--> 106\u001b[1;33m     \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"tokenizers/punkt/{0}.pickle\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\ada\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    750\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    751\u001b[0m     \u001b[1;31m# Load the resource.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 752\u001b[1;33m     \u001b[0mopened_resource\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    753\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    754\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"raw\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\ada\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    876\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"nltk\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 877\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    878\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"file\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m         \u001b[1;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\ada\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    583\u001b[0m     \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"*\"\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\\n%s\\n%s\\n%s\\n\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 585\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    586\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Younes/nltk_data'\n    - 'C:\\\\ProgramData\\\\Anaconda3\\\\envs\\\\ada\\\\nltk_data'\n    - 'C:\\\\ProgramData\\\\Anaconda3\\\\envs\\\\ada\\\\share\\\\nltk_data'\n    - 'C:\\\\ProgramData\\\\Anaconda3\\\\envs\\\\ada\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Younes\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "#Couting bigram frequencies for all articles of interest\n",
    "usNewsFEMbigramFreq = ngram_frequency(all_articles)\n",
    "\n",
    "MAX = 150\n",
    "\n",
    "#Visualize the most common bigrams\n",
    "for word, frequency in usNewsFEMbigramFreq.most_common(MAX):\n",
    "        print('%s;%d' % (word, frequency))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d229a76-5500-47ca-9179-3c74e27811c9",
   "metadata": {},
   "source": [
    "The most common bigrams list also contain a lot of Named Entities (NE) like cities, persons etc... \n",
    "Here we use the naive approach to ignore this name by using the word capitalization to select them. Note that there are more advanced way to recognize NE but we believe that it will be overkilled for our usage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea5b1b8-5af5-4d52-8769-6eee9f14636b",
   "metadata": {},
   "source": [
    "### Final List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff4ed09-03f4-47de-b49e-49168a9665ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_final_list = []\n",
    "MAX = 100\n",
    "for word, frequency in usNewsFEMbigramFreq.most_common(MAX):\n",
    "    if (word[0][0].isupper()==False and word[1][0].isupper()==False): #ignore the Named Entities\n",
    "        bigram_final_list.append(word[0]+' '+word[1])\n",
    "\n",
    "bigram_final_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c951416d-2481-4c93-be87-77a0090e43e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ToDo: Add comments about the one to avoid\n",
    "### ToDo: drop the useless one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8f6066-407f-4204-9e90-8b13570d905a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add my own vocabulary\n",
    "my_bigrams = ['women\\'s right','Equal opportunities','Equal rights','Equal status',\n",
    "           'equal pay','gender gap','Gender discrimination','Gender equality','Sexual harrasment','Women empowerment',\n",
    "            'women victim','women immigration','Women emancipation','women\\'s participation','Western women','non-western woman',\n",
    "              'Muslim women', 'Equal wages','Gender equality',\n",
    "             'gender equity','Men and women', 'women and men', 'women oppression', 'niqab ban'\n",
    "           'struggle of girls','struggle of women', 'war against women','oppression of girls','oppression of women',\n",
    "           'women oppression','women\\'s opression','liberate women','religious oppresion',\n",
    "           'abuse of women','Male oppression','Female oppression','Exploitation of women',\n",
    "           'Indigenous women','Patriarchal culture']\n",
    "\n",
    "all_bigrams = my_bigrams + bigram_final_list\n",
    "all_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b241b6-84b9-41fd-8f43-7539d9d243c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add the export"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d551a663-d2c4-4844-94db-6a8f4c2a3ed6",
   "metadata": {},
   "source": [
    "## Dataset selection from Quotebank database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea3d42c-938d-4915-b6f5-4426b078eed0",
   "metadata": {},
   "source": [
    "### Librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f9b793-f2e2-481f-8080-068996db859b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1d7808-e93a-4d3f-b9c1-42ca2756c8a8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "161934f4-7a89-4a52-a21d-9c2b9dc186ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processing on chunk\n",
    "#Input\n",
    "#Output\n",
    "def process_chunk(chunk, vocabulary):\n",
    "    print(f'Processing chunk with {len(chunk)} rows')\n",
    "    #print(chunk.columns)\n",
    "    occurences = np.zeros(len(vocabulary))\n",
    "    for index, word in enumerate(vocabulary):\n",
    "        occurences[index] = np.sum(chunk['quotation'].str.contains(word)) \n",
    "    return occurences\n",
    "\n",
    "#Select quotes containing keywords\n",
    "def select_quotes_chunk(chunk, keywords):\n",
    "    print(f'Processing chunk with {len(chunk)} rows')\n",
    "    return chunk[chunk['quotation'].str.contains('|'.join(keywords))]\n",
    "\n",
    "#Use the selection function on each chunk of the full dataset \n",
    "def select_quotes_one_year(path_to_file, vocabulary, chunksize = 10 ** 4):\n",
    "    with pd.read_json(path_to_file, lines=True, compression='bz2', chunksize=chunksize) as df_reader:\n",
    "        for index, chunk in enumerate(df_reader):\n",
    "            if not index==0:\n",
    "                selected_df = pd.concat([selected_df, select_quotes_chunk(chunk, vocabulary)])\n",
    "            else: \n",
    "                selected_df = select_quotes_chunk(chunk, vocabulary)\n",
    "    return selected_df\n",
    "\n",
    "#Use the selection function on each chunk of the full dataset \n",
    "#Dumps the selected quotes into a new json file\n",
    "def select_and_dump(path_to_file, vocabulary, chunksize = 10 ** 4, year = 'replace_me'):\n",
    "    with pd.read_json(path_to_file, lines=True, compression='bz2', chunksize=chunksize) as df_reader:\n",
    "        for index, chunk in enumerate(df_reader):\n",
    "            #Dump selected quotes\n",
    "            selected_df = select_quotes_chunk(chunk, vocabulary)\n",
    "            pickle_file_name = year + '_chunk_' + str(index) + '.pkl'\n",
    "            selected_df.to_pickle('files/'+pickle_file_name)\n",
    "            #if not index==0:\n",
    "                #selected_df = pd.concat([selected_df, select_quotes_chunk(chunk, vocabulary)])\n",
    "            #else: \n",
    "               # selected_df = select_quotes_chunk(chunk, vocabulary)\n",
    "    return selected_df\n",
    "\n",
    "\n",
    "import random, string\n",
    "\n",
    "def randomword(length):\n",
    "    letters = string.ascii_lowercase\n",
    "    return ''.join(random.choice(letters) for i in range(length))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6485950-baf8-4991-8f23-352efaddb36b",
   "metadata": {},
   "source": [
    "### Constants "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "027c0972-0ab2-4062-8e39-e3ed279c6e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add the import of keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27db09ff-680c-4ffd-b303-c36b6c012026",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = 'data/'\n",
    "QUOTEBANK_2020 = DATA_FOLDER+ \"quotes-2020.json.bz2\"\n",
    "QUOTEBANK_2019 = DATA_FOLDER+ \"quotes-2019.json.bz2\"\n",
    "QUOTEBANK_2017 = DATA_FOLDER+ \"quotes-2017.json.bz2\"\n",
    "QUOTEBANK_2015 = DATA_FOLDER+ \"quotes-2015.json.bz2\"\n",
    "QUOTEBANK_2018 = DATA_FOLDER+ \"quotes-2018.json.bz2\"\n",
    "QUOTEBANK_2016 = DATA_FOLDER+ \"quotes-2016.json.bz2\"\n",
    "\n",
    "PATH = 'generated_data/'\n",
    "\n",
    "PARQUET_FILE = PATH +  \"speaker_attributes.parquet\"\n",
    "\n",
    "KEYWORDS_LIST = ('women\\'s right','Equal opportunities','Equal rights','Equal status','equal pay',\n",
    "              'gender gap','Gender discrimination','Gender equality','Sexual harrassment',\n",
    "              'Women empowerment','women victim','women immigration','Women emancipation',\n",
    "              'women\\'s participation','Western women','non-western woman','Muslim women',\n",
    "              'Equal wages','Gender equality','gender equity','Men and women','women and men',\n",
    "              'women oppression','niqab ban','struggle of girls','struggle of women','war against women',\n",
    "              'oppression of girls','oppression of women','women oppression','women\\'s opression','liberate women',\n",
    "              'religious oppresion','abuse of women','Male oppression','Female oppression','Exploitation of women',\n",
    "              'Indigenous women','Patriarchal culture','gender equality','child care','men pay','percentage men',\n",
    "              'pay percentage','sexual harassment','women girls','girls women',\n",
    "              'rates women','women according','female mayors','share women','women movement',\n",
    "              'see women','gender stereotypes','gender gap',\n",
    "              'women representation','sex discrimination','states women','lose weight',\n",
    "              'women rights','woman time',\n",
    "              'based gender',\n",
    "              'proportional electoral','female candidates','gender-based violence','entirely female','cities female')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18335ed-86e8-4b6a-8088-8c6806ffa113",
   "metadata": {},
   "source": [
    "### Select and pickle of quotes of interest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fbcb35-0447-4748-abc8-6b0877a96da9",
   "metadata": {},
   "source": [
    "Note: This code has to be run once to create the pickle files containing the quotes of interest. For futher use, the dataframe is direcly loaded from the pickle files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73091be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %time QOI_2015_DF = select_quotes_one_year(QUOTEBANK_2015,KEYWORDS_LIST,10 ** 4)\n",
    "# %time QOI_2016_DF = select_quotes_one_year(QUOTEBANK_2016,KEYWORDS_LIST,10 ** 4)\n",
    "# %time QOI_2017_DF = select_quotes_one_year(QUOTEBANK_2017,KEYWORDS_LIST,10 ** 4)\n",
    "# %time QOI_2018_DF = select_quotes_one_year(QUOTEBANK_2018,KEYWORDS_LIST,10 ** 4)\n",
    "# %time QOI_2019_DF = select_quotes_one_year(QUOTEBANK_2019,KEYWORDS_LIST,10 ** 4)\n",
    "# %time QOI_2020_DF = select_quotes_one_year(QUOTEBANK_2020,KEYWORDS_LIST,10 ** 4)\n",
    "\n",
    "dataframesNames = ('QOI_2015_DF','QOI_2016_DF','QOI_2017_DF','QOI_2018_DF','QOI_2019_DF','QOI_2020_DF')\n",
    "#dataframes = (QOI_2015_DF,QOI_2016_DF,QOI_2017_DF,QOI_2018_DF,QOI_2019_DF,QOI_2020_DF)\n",
    "\n",
    "#for i in range(len(dataframesNames)):\n",
    "#    dataframes[i].to_pickle('generated_data/'+dataframesNames[i]+'.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37178ef-3230-4ebf-9f66-5e88bf2ff4c0",
   "metadata": {},
   "source": [
    "### Load dataframe of selected quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75e117f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d2474a4-0ab1-46e3-8aa5-8c6445b5e59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate into one dataframes the dataframes from each pickle file. \n",
    "df = pd.concat([pd.read_pickle(PATH+ fp +'.pkl') for fp in dataframesNames], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14ea9e5f-fcf3-4d6a-b111-fe2cff46806b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quoteID</th>\n",
       "      <th>quotation</th>\n",
       "      <th>speaker</th>\n",
       "      <th>qids</th>\n",
       "      <th>date</th>\n",
       "      <th>numOccurrences</th>\n",
       "      <th>probas</th>\n",
       "      <th>urls</th>\n",
       "      <th>phase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-03-09-004706</td>\n",
       "      <td>Anything less than women winning 50 per cent o...</td>\n",
       "      <td>Katy Gallagher</td>\n",
       "      <td>[Q463507]</td>\n",
       "      <td>2015-03-09 12:30:00</td>\n",
       "      <td>1</td>\n",
       "      <td>[[Katy Gallagher, 0.5872], [None, 0.4128]]</td>\n",
       "      <td>[http://www.smh.com.au/act-news/women-need-to-...</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-04-24-025718</td>\n",
       "      <td>I'd like to congratulate all the winners and f...</td>\n",
       "      <td>Helena Morrissey</td>\n",
       "      <td>[Q23762081]</td>\n",
       "      <td>2015-04-24 15:33:00</td>\n",
       "      <td>1</td>\n",
       "      <td>[[Helena Morrissey, 0.8706], [None, 0.1294]]</td>\n",
       "      <td>[http://www.cipd.co.uk/PM/peoplemanagement/b/w...</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-07-16-044620</td>\n",
       "      <td>I think what Deepika has spoken in the video m...</td>\n",
       "      <td>Kalki Koechlin</td>\n",
       "      <td>[Q3192216]</td>\n",
       "      <td>2015-07-16 16:41:07</td>\n",
       "      <td>1</td>\n",
       "      <td>[[Kalki Koechlin, 0.6377], [None, 0.3623]]</td>\n",
       "      <td>[http://www.pinkvilla.com/entertainmenttags/ka...</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-09-11-052815</td>\n",
       "      <td>if advocating for equal pay for equal work is ...</td>\n",
       "      <td>Hillary Clinton</td>\n",
       "      <td>[Q6294]</td>\n",
       "      <td>2015-09-11 14:17:08</td>\n",
       "      <td>1</td>\n",
       "      <td>[[Hillary Clinton, 0.8831], [None, 0.1105], [D...</td>\n",
       "      <td>[http://www.wrn.com/2015/09/hillary-clinton-ra...</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-04-23-037713</td>\n",
       "      <td>Men and women are understandably upset if they...</td>\n",
       "      <td>Jim McDermott</td>\n",
       "      <td>[Q321457, Q6196778]</td>\n",
       "      <td>2015-04-23 21:52:22</td>\n",
       "      <td>1</td>\n",
       "      <td>[[Jim McDermott, 0.629], [John F. Kerry, 0.190...</td>\n",
       "      <td>[http://www.atlanticcouncil.org/en/blogs/new-a...</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             quoteID                                          quotation  \\\n",
       "0  2015-03-09-004706  Anything less than women winning 50 per cent o...   \n",
       "1  2015-04-24-025718  I'd like to congratulate all the winners and f...   \n",
       "2  2015-07-16-044620  I think what Deepika has spoken in the video m...   \n",
       "3  2015-09-11-052815  if advocating for equal pay for equal work is ...   \n",
       "4  2015-04-23-037713  Men and women are understandably upset if they...   \n",
       "\n",
       "            speaker                 qids                date  numOccurrences  \\\n",
       "0    Katy Gallagher            [Q463507] 2015-03-09 12:30:00               1   \n",
       "1  Helena Morrissey          [Q23762081] 2015-04-24 15:33:00               1   \n",
       "2    Kalki Koechlin           [Q3192216] 2015-07-16 16:41:07               1   \n",
       "3   Hillary Clinton              [Q6294] 2015-09-11 14:17:08               1   \n",
       "4     Jim McDermott  [Q321457, Q6196778] 2015-04-23 21:52:22               1   \n",
       "\n",
       "                                              probas  \\\n",
       "0         [[Katy Gallagher, 0.5872], [None, 0.4128]]   \n",
       "1       [[Helena Morrissey, 0.8706], [None, 0.1294]]   \n",
       "2         [[Kalki Koechlin, 0.6377], [None, 0.3623]]   \n",
       "3  [[Hillary Clinton, 0.8831], [None, 0.1105], [D...   \n",
       "4  [[Jim McDermott, 0.629], [John F. Kerry, 0.190...   \n",
       "\n",
       "                                                urls phase  \n",
       "0  [http://www.smh.com.au/act-news/women-need-to-...     E  \n",
       "1  [http://www.cipd.co.uk/PM/peoplemanagement/b/w...     E  \n",
       "2  [http://www.pinkvilla.com/entertainmenttags/ka...     E  \n",
       "3  [http://www.wrn.com/2015/09/hillary-clinton-ra...     E  \n",
       "4  [http://www.atlanticcouncil.org/en/blogs/new-a...     E  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0bcc2c3b-9b79-4519-b6aa-7f3d9aa4a476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataframe has 87161 entries\n"
     ]
    }
   ],
   "source": [
    "print(f'The dataframe has {len(df)} entries')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eab691c9-49dd-4ec3-a9ca-8b428c448731",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Anything less than women winning 50 per cent of new seats will be a loss not only for a progressive city's progress towards true gender equality but it would also be a loss for good governance in ...\n",
       "1    I'd like to congratulate all the winners and finalists on their success. They have demonstrated clear leadership by moving women's progression from a `diversity' initiative to a core business prio...\n",
       "2    I think what Deepika has spoken in the video makes sense. I do understand the counter argument too where everyone has been saying that had men said the same lines about having sex outside marriage...\n",
       "3                                                                                    if advocating for equal pay for equal work is playing the gender card, deal me in. I am ready to play as hard as I can.\n",
       "4      Men and women are understandably upset if they see a company close down and jobs lost. It's only natural people would look around and in their distress they find something or someone able to blame,\n",
       "Name: quotation, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_colwidth = 200\n",
    "df.head()['quotation']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556e8468",
   "metadata": {},
   "source": [
    "## Enriching the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cc366f",
   "metadata": {},
   "source": [
    "### Wikidata : Using QIDs to obtain attributes for a speaker, then making the attributes readable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30379d91",
   "metadata": {},
   "source": [
    "The wikidata knowledge base contains a lot of information we can leverage t obtain extra information on the selected speakers. First, We look at the QID column of the selected quote dataframe : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "31648f2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                  [Q463507]\n",
       "1                [Q23762081]\n",
       "2                 [Q3192216]\n",
       "3                    [Q6294]\n",
       "4        [Q321457, Q6196778]\n",
       "                ...         \n",
       "87156                     []\n",
       "87157            [Q56678515]\n",
       "87158                     []\n",
       "87159             [Q7352658]\n",
       "87160               [Q57701]\n",
       "Name: qids, Length: 87161, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['qids']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2133fc26",
   "metadata": {},
   "source": [
    "As we can see in the fourth row sometimes multiple QIDs are present, this is usually because there many entries on wikidata for the same speaker name. In the following, whenever there is a column containing multiple QIDs only the first QID is kept. This is for simplicity reason, but in the future will be changed as it would be interesting for our analyses to keep all the QID entries for attributes like the political party."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471156a5",
   "metadata": {},
   "source": [
    "The scripts used to convert the qids to label are present in the `scripts` folder in the `qid_to_label.py` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ec19f137",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.qid_to_label import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46acc3f7",
   "metadata": {},
   "source": [
    "First  the .parquet file containing attributes contining the labels attaches to a qid is loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6938f0bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 20.8 s\n"
     ]
    }
   ],
   "source": [
    "%time parquet_df = pd.read_parquet(PARQUET_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2043af07",
   "metadata": {},
   "source": [
    "We then join it to dataframe containing the selected quotes, using the qid column to perform a left join with the parquet file on the right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "88220bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Younes\\Desktop\\P2\\scripts\\qid_to_label.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['qids'] = df['qids'].apply(lambda x: x[0]) # this gives a warning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 10.3 s\n"
     ]
    }
   ],
   "source": [
    "%time merged_df = merge_df(df,parquet_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7198c819",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quoteID</th>\n",
       "      <th>quotation</th>\n",
       "      <th>speaker</th>\n",
       "      <th>qids</th>\n",
       "      <th>date</th>\n",
       "      <th>numOccurrences</th>\n",
       "      <th>probas</th>\n",
       "      <th>urls</th>\n",
       "      <th>phase</th>\n",
       "      <th>aliases</th>\n",
       "      <th>...</th>\n",
       "      <th>ethnic_group</th>\n",
       "      <th>US_congress_bio_ID</th>\n",
       "      <th>occupation</th>\n",
       "      <th>party</th>\n",
       "      <th>academic_degree</th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>candidacy</th>\n",
       "      <th>type</th>\n",
       "      <th>religion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-03-09-004706</td>\n",
       "      <td>Anything less than women winning 50 per cent of new seats will be a loss not only for a progressive city's progress towards true gender equality but it would also be a loss for good governance in ...</td>\n",
       "      <td>Katy Gallagher</td>\n",
       "      <td>Q463507</td>\n",
       "      <td>2015-03-09 12:30:00</td>\n",
       "      <td>1</td>\n",
       "      <td>[[Katy Gallagher, 0.5872], [None, 0.4128]]</td>\n",
       "      <td>[http://www.smh.com.au/act-news/women-need-to-stand-for-election-in-the-act-katy-gallagher-20150309-13yuz2.html]</td>\n",
       "      <td>E</td>\n",
       "      <td>[Katherine Gallagher, Katherine Ruth Gallagher]</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[Q82955, Q7019111, Q15627169]</td>\n",
       "      <td>[Q216082]</td>\n",
       "      <td>None</td>\n",
       "      <td>Q463507</td>\n",
       "      <td>Katy Gallagher</td>\n",
       "      <td>None</td>\n",
       "      <td>item</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-05-11-081817</td>\n",
       "      <td>more family-friendly and flexible workplaces, and affordable child care, for everyone</td>\n",
       "      <td>Katy Gallagher</td>\n",
       "      <td>Q463507</td>\n",
       "      <td>2017-05-11 00:00:37</td>\n",
       "      <td>1</td>\n",
       "      <td>[[Katy Gallagher, 0.5596], [None, 0.4404]]</td>\n",
       "      <td>[http://www.mediamaxnetwork.co.ke/news/325569/senator-breastfeeds-australian-parliament/]</td>\n",
       "      <td>E</td>\n",
       "      <td>[Katherine Gallagher, Katherine Ruth Gallagher]</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[Q82955, Q7019111, Q15627169]</td>\n",
       "      <td>[Q216082]</td>\n",
       "      <td>None</td>\n",
       "      <td>Q463507</td>\n",
       "      <td>Katy Gallagher</td>\n",
       "      <td>None</td>\n",
       "      <td>item</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-06-23-130636</td>\n",
       "      <td>We need more women and parents in Parliament. And we need more family-friendly and flexible workplaces, and affordable child care, for everyone.</td>\n",
       "      <td>Katy Gallagher</td>\n",
       "      <td>Q463507</td>\n",
       "      <td>2017-06-23 03:20:00</td>\n",
       "      <td>1</td>\n",
       "      <td>[[Katy Gallagher, 0.4996], [None, 0.4403], [Larissa Waters, 0.0601]]</td>\n",
       "      <td>[http://www.harpersbazaar.com/culture/features/a10212753/australian-politician-breastfeeding-baby-parliament/]</td>\n",
       "      <td>E</td>\n",
       "      <td>[Katherine Gallagher, Katherine Ruth Gallagher]</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[Q82955, Q7019111, Q15627169]</td>\n",
       "      <td>[Q216082]</td>\n",
       "      <td>None</td>\n",
       "      <td>Q463507</td>\n",
       "      <td>Katy Gallagher</td>\n",
       "      <td>None</td>\n",
       "      <td>item</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             quoteID  \\\n",
       "0  2015-03-09-004706   \n",
       "1  2017-05-11-081817   \n",
       "2  2017-06-23-130636   \n",
       "\n",
       "                                                                                                                                                                                                 quotation  \\\n",
       "0  Anything less than women winning 50 per cent of new seats will be a loss not only for a progressive city's progress towards true gender equality but it would also be a loss for good governance in ...   \n",
       "1                                                                                                                    more family-friendly and flexible workplaces, and affordable child care, for everyone   \n",
       "2                                                         We need more women and parents in Parliament. And we need more family-friendly and flexible workplaces, and affordable child care, for everyone.   \n",
       "\n",
       "          speaker     qids                date  numOccurrences  \\\n",
       "0  Katy Gallagher  Q463507 2015-03-09 12:30:00               1   \n",
       "1  Katy Gallagher  Q463507 2017-05-11 00:00:37               1   \n",
       "2  Katy Gallagher  Q463507 2017-06-23 03:20:00               1   \n",
       "\n",
       "                                                                 probas  \\\n",
       "0                            [[Katy Gallagher, 0.5872], [None, 0.4128]]   \n",
       "1                            [[Katy Gallagher, 0.5596], [None, 0.4404]]   \n",
       "2  [[Katy Gallagher, 0.4996], [None, 0.4403], [Larissa Waters, 0.0601]]   \n",
       "\n",
       "                                                                                                               urls  \\\n",
       "0  [http://www.smh.com.au/act-news/women-need-to-stand-for-election-in-the-act-katy-gallagher-20150309-13yuz2.html]   \n",
       "1                         [http://www.mediamaxnetwork.co.ke/news/325569/senator-breastfeeds-australian-parliament/]   \n",
       "2    [http://www.harpersbazaar.com/culture/features/a10212753/australian-politician-breastfeeding-baby-parliament/]   \n",
       "\n",
       "  phase                                          aliases  ... ethnic_group  \\\n",
       "0     E  [Katherine Gallagher, Katherine Ruth Gallagher]  ...         None   \n",
       "1     E  [Katherine Gallagher, Katherine Ruth Gallagher]  ...         None   \n",
       "2     E  [Katherine Gallagher, Katherine Ruth Gallagher]  ...         None   \n",
       "\n",
       "  US_congress_bio_ID                     occupation      party  \\\n",
       "0               None  [Q82955, Q7019111, Q15627169]  [Q216082]   \n",
       "1               None  [Q82955, Q7019111, Q15627169]  [Q216082]   \n",
       "2               None  [Q82955, Q7019111, Q15627169]  [Q216082]   \n",
       "\n",
       "  academic_degree       id           label candidacy  type religion  \n",
       "0            None  Q463507  Katy Gallagher      None  item     None  \n",
       "1            None  Q463507  Katy Gallagher      None  item     None  \n",
       "2            None  Q463507  Katy Gallagher      None  item     None  \n",
       "\n",
       "[3 rows x 24 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b16cc7e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum number of QID entry for each of the new columns\n",
      "nationality : 5\n",
      "gender : 2\n",
      "ethnic_group : 6\n",
      "US_congress_bio_ID : 7\n",
      "occupation : 24\n",
      "party : 9\n",
      "academic_degree : 4\n",
      "id : 9\n",
      "label : 50\n",
      "candidacy : 13\n",
      "type : 4\n",
      "religion : 7\n"
     ]
    }
   ],
   "source": [
    "print('maximum number of QID entry for each of the new columns')\n",
    "print('nationality :', merged_df['nationality'].apply(lambda x : len(x) if x is not None else 0).max())\n",
    "print('gender :', merged_df['gender'].apply(lambda x : len(x) if x is not None else 0).max())\n",
    "print('ethnic_group :', merged_df['ethnic_group'].apply(lambda x : len(x) if x is not None else 0).max())\n",
    "print('US_congress_bio_ID :', merged_df['US_congress_bio_ID'].apply(lambda x : len(x) if x is not None else 0).max())\n",
    "print('occupation :', merged_df['occupation'].apply(lambda x : len(x) if x is not None else 0).max())\n",
    "print('party :', merged_df['party'].apply(lambda x : len(x) if x is not None else 0).max())\n",
    "print('academic_degree :', merged_df['academic_degree'].apply(lambda x : len(x) if x is not None else 0).max())\n",
    "print('id :', merged_df['id'].apply(lambda x : len(x) if x is not None else 0).max())\n",
    "print('label :', merged_df['label'].apply(lambda x : len(x) if x is not None else 0).max())\n",
    "print('candidacy :', merged_df['candidacy'].apply(lambda x : len(x) if x is not None else 0).max())\n",
    "print('type :', merged_df['type'].apply(lambda x : len(x) if x is not None else 0).max())\n",
    "print('religion :', merged_df['religion'].apply(lambda x : len(x) if x is not None else 0).max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a575fe",
   "metadata": {},
   "source": [
    "Additionally above we see a lot of columns contain multiple entries (a list of QID), for the ones containing QIDs, only the first entry is kept (first element of the list of QIDS)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0dbd84",
   "metadata": {},
   "source": [
    "Instead of manually checking we use library re to scan every column if there are QIDs. \n",
    "Most columns are composed of a numpy nd.array containing a tuple of strings, every string being a qid. So we first access the tuple, then take the first element, then check if it's a QID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d3bec545",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0722bdc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_qid(df):\n",
    "    '''\n",
    "    take first term of list out of ndarray, and checks if it's a QID\n",
    "    '''\n",
    "    return df.apply(lambda x: x[0] if x is not None else None).str.contains(r'[Q][0-9]+').any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8973eb69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column : nationality  True\n",
      "column : gender  True\n",
      "column : ethnic_group  True\n",
      "column : US_congress_bio_ID  False\n",
      "column : occupation  True\n",
      "column : party  True\n",
      "column : academic_degree  True\n",
      "column : id  False\n",
      "column : label  False\n",
      "column : candidacy  True\n",
      "column : type  False\n",
      "column : religion  True\n"
     ]
    }
   ],
   "source": [
    "print('column : nationality ', check_if_qid(merged_df['nationality']))\n",
    "print('column : gender ', check_if_qid(merged_df['gender']))\n",
    "print('column : ethnic_group ', check_if_qid(merged_df['ethnic_group']))\n",
    "print('column : US_congress_bio_ID ', check_if_qid(merged_df['US_congress_bio_ID']))\n",
    "print('column : occupation ', check_if_qid(merged_df['occupation']))\n",
    "print('column : party ', check_if_qid(merged_df['party']))\n",
    "print('column : academic_degree ', check_if_qid(merged_df['academic_degree']))\n",
    "print('column : id ', check_if_qid(merged_df['id']))\n",
    "print('column : label ', check_if_qid(merged_df['label']))\n",
    "print('column : candidacy ', check_if_qid(merged_df['candidacy']))\n",
    "print('column : type ', check_if_qid(merged_df['type']))\n",
    "print('column : religion ', check_if_qid(merged_df['religion']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83faf919",
   "metadata": {},
   "source": [
    "We can see that some attributes are still in QID format so aren't readable, to translate them to readable labels we have two options, either `wikidata_labels_descriptions_quotebank.csv.bz2` or `wikidata_labels_descriptions.csv.bz2`. We will initally use the first option since it should contain all the labels we need for Quotebank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "21a871cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "qids_onlyquotebank_path = PATH + \"wikidata_labels_descriptions_quotebank.csv.bz2\"\n",
    "column_names = ('nationality', 'gender', 'ethnic_group', 'occupation', 'party', 'academic_degree', 'religion')\n",
    "merged_df = qid_to_label(merged_df, qids_onlyquotebank_path, column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5fb1ef24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quoteID</th>\n",
       "      <th>quotation</th>\n",
       "      <th>speaker</th>\n",
       "      <th>qids</th>\n",
       "      <th>date</th>\n",
       "      <th>numOccurrences</th>\n",
       "      <th>probas</th>\n",
       "      <th>urls</th>\n",
       "      <th>phase</th>\n",
       "      <th>aliases</th>\n",
       "      <th>...</th>\n",
       "      <th>ethnic_group</th>\n",
       "      <th>US_congress_bio_ID</th>\n",
       "      <th>occupation</th>\n",
       "      <th>party</th>\n",
       "      <th>academic_degree</th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>candidacy</th>\n",
       "      <th>type</th>\n",
       "      <th>religion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-03-09-004706</td>\n",
       "      <td>Anything less than women winning 50 per cent of new seats will be a loss not only for a progressive city's progress towards true gender equality but it would also be a loss for good governance in ...</td>\n",
       "      <td>Katy Gallagher</td>\n",
       "      <td>Q463507</td>\n",
       "      <td>2015-03-09 12:30:00</td>\n",
       "      <td>1</td>\n",
       "      <td>[[Katy Gallagher, 0.5872], [None, 0.4128]]</td>\n",
       "      <td>[http://www.smh.com.au/act-news/women-need-to-stand-for-election-in-the-act-katy-gallagher-20150309-13yuz2.html]</td>\n",
       "      <td>E</td>\n",
       "      <td>[Katherine Gallagher, Katherine Ruth Gallagher]</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>politician</td>\n",
       "      <td>Australian Labor Party</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q463507</td>\n",
       "      <td>Katy Gallagher</td>\n",
       "      <td>None</td>\n",
       "      <td>item</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-05-11-081817</td>\n",
       "      <td>more family-friendly and flexible workplaces, and affordable child care, for everyone</td>\n",
       "      <td>Katy Gallagher</td>\n",
       "      <td>Q463507</td>\n",
       "      <td>2017-05-11 00:00:37</td>\n",
       "      <td>1</td>\n",
       "      <td>[[Katy Gallagher, 0.5596], [None, 0.4404]]</td>\n",
       "      <td>[http://www.mediamaxnetwork.co.ke/news/325569/senator-breastfeeds-australian-parliament/]</td>\n",
       "      <td>E</td>\n",
       "      <td>[Katherine Gallagher, Katherine Ruth Gallagher]</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>politician</td>\n",
       "      <td>Australian Labor Party</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q463507</td>\n",
       "      <td>Katy Gallagher</td>\n",
       "      <td>None</td>\n",
       "      <td>item</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-06-23-130636</td>\n",
       "      <td>We need more women and parents in Parliament. And we need more family-friendly and flexible workplaces, and affordable child care, for everyone.</td>\n",
       "      <td>Katy Gallagher</td>\n",
       "      <td>Q463507</td>\n",
       "      <td>2017-06-23 03:20:00</td>\n",
       "      <td>1</td>\n",
       "      <td>[[Katy Gallagher, 0.4996], [None, 0.4403], [Larissa Waters, 0.0601]]</td>\n",
       "      <td>[http://www.harpersbazaar.com/culture/features/a10212753/australian-politician-breastfeeding-baby-parliament/]</td>\n",
       "      <td>E</td>\n",
       "      <td>[Katherine Gallagher, Katherine Ruth Gallagher]</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>politician</td>\n",
       "      <td>Australian Labor Party</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q463507</td>\n",
       "      <td>Katy Gallagher</td>\n",
       "      <td>None</td>\n",
       "      <td>item</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-04-24-025718</td>\n",
       "      <td>I'd like to congratulate all the winners and finalists on their success. They have demonstrated clear leadership by moving women's progression from a `diversity' initiative to a core business prio...</td>\n",
       "      <td>Helena Morrissey</td>\n",
       "      <td>Q23762081</td>\n",
       "      <td>2015-04-24 15:33:00</td>\n",
       "      <td>1</td>\n",
       "      <td>[[Helena Morrissey, 0.8706], [None, 0.1294]]</td>\n",
       "      <td>[http://www.cipd.co.uk/PM/peoplemanagement/b/weblog/archive/2015/04/24/winners-unveiled-at-opportunity-now-s-women-and-work-awards-2015.aspx]</td>\n",
       "      <td>E</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>business executive</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q23762081</td>\n",
       "      <td>Helena Morrissey</td>\n",
       "      <td>None</td>\n",
       "      <td>item</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-04-08-011609</td>\n",
       "      <td>Clearly we have got a long way to go before we have true gender equality at all levels,</td>\n",
       "      <td>Helena Morrissey</td>\n",
       "      <td>Q23762081</td>\n",
       "      <td>2015-04-08 18:28:01</td>\n",
       "      <td>1</td>\n",
       "      <td>[[Helena Morrissey, 0.5805], [None, 0.2061], [Vince Cable, 0.1981], [Mervyn Davies, 0.0153]]</td>\n",
       "      <td>[http://feeds.theguardian.com/c/34708/f/663879/s/4538fae0/sc/28/l/0L0Stheguardian0N0Cbusiness0C20A150Capr0C0A80Cgreater0Eeconomic0Eand0Epolitical0Epower0Egives0Ewomen0Emore0Eseats0Ein0Eboadroom/st...</td>\n",
       "      <td>E</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>business executive</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Q23762081</td>\n",
       "      <td>Helena Morrissey</td>\n",
       "      <td>None</td>\n",
       "      <td>item</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             quoteID  \\\n",
       "0  2015-03-09-004706   \n",
       "1  2017-05-11-081817   \n",
       "2  2017-06-23-130636   \n",
       "3  2015-04-24-025718   \n",
       "4  2015-04-08-011609   \n",
       "\n",
       "                                                                                                                                                                                                 quotation  \\\n",
       "0  Anything less than women winning 50 per cent of new seats will be a loss not only for a progressive city's progress towards true gender equality but it would also be a loss for good governance in ...   \n",
       "1                                                                                                                    more family-friendly and flexible workplaces, and affordable child care, for everyone   \n",
       "2                                                         We need more women and parents in Parliament. And we need more family-friendly and flexible workplaces, and affordable child care, for everyone.   \n",
       "3  I'd like to congratulate all the winners and finalists on their success. They have demonstrated clear leadership by moving women's progression from a `diversity' initiative to a core business prio...   \n",
       "4                                                                                                                  Clearly we have got a long way to go before we have true gender equality at all levels,   \n",
       "\n",
       "            speaker       qids                date  numOccurrences  \\\n",
       "0    Katy Gallagher    Q463507 2015-03-09 12:30:00               1   \n",
       "1    Katy Gallagher    Q463507 2017-05-11 00:00:37               1   \n",
       "2    Katy Gallagher    Q463507 2017-06-23 03:20:00               1   \n",
       "3  Helena Morrissey  Q23762081 2015-04-24 15:33:00               1   \n",
       "4  Helena Morrissey  Q23762081 2015-04-08 18:28:01               1   \n",
       "\n",
       "                                                                                         probas  \\\n",
       "0                                                    [[Katy Gallagher, 0.5872], [None, 0.4128]]   \n",
       "1                                                    [[Katy Gallagher, 0.5596], [None, 0.4404]]   \n",
       "2                          [[Katy Gallagher, 0.4996], [None, 0.4403], [Larissa Waters, 0.0601]]   \n",
       "3                                                  [[Helena Morrissey, 0.8706], [None, 0.1294]]   \n",
       "4  [[Helena Morrissey, 0.5805], [None, 0.2061], [Vince Cable, 0.1981], [Mervyn Davies, 0.0153]]   \n",
       "\n",
       "                                                                                                                                                                                                      urls  \\\n",
       "0                                                                                         [http://www.smh.com.au/act-news/women-need-to-stand-for-election-in-the-act-katy-gallagher-20150309-13yuz2.html]   \n",
       "1                                                                                                                [http://www.mediamaxnetwork.co.ke/news/325569/senator-breastfeeds-australian-parliament/]   \n",
       "2                                                                                           [http://www.harpersbazaar.com/culture/features/a10212753/australian-politician-breastfeeding-baby-parliament/]   \n",
       "3                                                            [http://www.cipd.co.uk/PM/peoplemanagement/b/weblog/archive/2015/04/24/winners-unveiled-at-opportunity-now-s-women-and-work-awards-2015.aspx]   \n",
       "4  [http://feeds.theguardian.com/c/34708/f/663879/s/4538fae0/sc/28/l/0L0Stheguardian0N0Cbusiness0C20A150Capr0C0A80Cgreater0Eeconomic0Eand0Epolitical0Epower0Egives0Ewomen0Emore0Eseats0Ein0Eboadroom/st...   \n",
       "\n",
       "  phase                                          aliases  ... ethnic_group  \\\n",
       "0     E  [Katherine Gallagher, Katherine Ruth Gallagher]  ...          NaN   \n",
       "1     E  [Katherine Gallagher, Katherine Ruth Gallagher]  ...          NaN   \n",
       "2     E  [Katherine Gallagher, Katherine Ruth Gallagher]  ...          NaN   \n",
       "3     E                                             None  ...          NaN   \n",
       "4     E                                             None  ...          NaN   \n",
       "\n",
       "  US_congress_bio_ID          occupation                   party  \\\n",
       "0               None          politician  Australian Labor Party   \n",
       "1               None          politician  Australian Labor Party   \n",
       "2               None          politician  Australian Labor Party   \n",
       "3               None  business executive                     NaN   \n",
       "4               None  business executive                     NaN   \n",
       "\n",
       "  academic_degree         id             label candidacy  type religion  \n",
       "0             NaN    Q463507    Katy Gallagher      None  item      NaN  \n",
       "1             NaN    Q463507    Katy Gallagher      None  item      NaN  \n",
       "2             NaN    Q463507    Katy Gallagher      None  item      NaN  \n",
       "3             NaN  Q23762081  Helena Morrissey      None  item      NaN  \n",
       "4             NaN  Q23762081  Helena Morrissey      None  item      NaN  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb865953",
   "metadata": {},
   "source": [
    "Finally we check one last time if all the QIDs have been replaced :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bb1baf30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column : nationality  False\n",
      "column : gender  False\n",
      "column : ethnic_group  False\n",
      "column : US_congress_bio_ID  False\n",
      "column : occupation  False\n",
      "column : party  False\n",
      "column : academic_degree  False\n",
      "column : id  True\n",
      "column : label  False\n",
      "column : candidacy  False\n",
      "column : type  False\n",
      "column : religion  False\n"
     ]
    }
   ],
   "source": [
    "check_df_columns_if_qid(merged_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a636955",
   "metadata": {},
   "source": [
    "Just incase the `wikidata_labels_descriptions_quotebank.csv.bz2` file didn't contain all the QID we needed the `process_qid_one(path_to_file, qids_clean, chunksize = 10 ** 4)` function can be use to translate the QIDS to labels using the larger file `wikidata_labels_descriptions.csv.bz2` containing all the wikidata QIDs and their corresponding labels / description. The function was made to be able to use chunks of the larger file containing the labels, and do left join on every new chunk while keeping the previous merged dataframe."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
