{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Milestone2 - Group Concatsanddogs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The use of women's rights and gender equality rhetoric in the US"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Header"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is organized followings the different steps used in our pipeline. We first create a list of keywords using [web scraping](#Webscraping) and a personal list of keywords. \n",
    "With this list of keywords we [select](#Dataset-selection-from-Quotebank-database) a subset of the Quotebank database. This subset will be our starting dataset for our project. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Webscraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to scrape the **usnews.com** website because they have some topic pages that list all articles on the specified topic.   \n",
    "For example this page https://www.usnews.com/topics/subjects/feminism identifies all the articles from usnews.com that are relevant on the topic of feminism in *the latest* column. Further down we will call these topic pages, primary URLs. We then access all the identified articles on their urls, i.e. secondary urls, and retrieve their contents to create a corpus of text relevant to our topic. The corpus is saved in *Articles_Contents.txt.*   \n",
    "The corpus is used to retrieve bigrams. We decided to not count onegrame because they are too general for our purpose, for example 'women' gives a lot of results but isn't always of interest. The following quote \"a woman, a woman, a woman.\" from an unknown speaker isn't relevant for our purpose.\n",
    "The bigrams complete a manual list of keywords that are used to select our quotes of interest. The web-scraped keywords are necessary to ensure that we don't miss frequent bigrams and to remove some of the bias that exist in a personal keywords list.\n",
    "Note that at this point we used only usnews as a source and we were not able to mimick the infinite scrolling so only a limited list of articles per topic is available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.7) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/aminamatt/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json \n",
    "import requests #http library\n",
    "import nltk #natural language processing library\n",
    "nltk.download('stopwords') #common english words to ignore \n",
    "from bs4 import BeautifulSoup #extraction from HTML and XML files\n",
    "from collections import Counter #dictionary subclass for counting hashable objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Description: Retrieving of urls of articles from a topic page of usnews.com\n",
    "#Input: the primary URL string, i.e. the URL with the list of relevant articles\n",
    "#Output: a list of urls strings referring to relevant articles \n",
    "#Requirements : Request, BeautifulSoup libraries\n",
    "#Use: This function is made to be used to scrap the to usnews.com website. \n",
    "#If you want to adapt it to another website the class tag should be adapted.\n",
    "def get_urls_usnews(URL):\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0 (X11; CrOS x86_64 12871.102.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.141 Safari/537.36\"}\n",
    "    response = requests.get(URL,headers=headers) #http request with a user-agent string to avoid blocking from server\n",
    "    soup = BeautifulSoup(response.text, 'html.parser') #parse the document with html format\n",
    "    latest = soup.find('div',{'class':\"LoadMoreWrapper__Container-zwyk5c-0 himujt\"}) #get all the elements within 'the latest'category\n",
    "    #Find all the urls in the articles of latest category\n",
    "    list_of_urls = []\n",
    "    for a in latest.find_all('a'):\n",
    "        list_of_urls.append(a['href'])\n",
    "    usnews_urls = list(set(list_of_urls))\n",
    "    \n",
    "    return usnews_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Description: Retrieving article content from a url and cleaning out the copyright mention\n",
    "#Input: url string of a single article\n",
    "#Output: string with all the article text\n",
    "#Requirements : Requests, BeautifulSoup,Json\n",
    "#Use: This function is made to be used to scrap the to usnews.com website. \n",
    "#If you want to adapt it to another website the copyright sentence should be adapted.\n",
    "def article_from_url(url):\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0 (X11; CrOS x86_64 12871.102.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.141 Safari/537.36\"}\n",
    "    response = requests.get(url,headers=headers) #http request with a user-agent string to avoid blocking from server\n",
    "    soup = BeautifulSoup(response.text, 'html.parser') #parse the document with html format\n",
    "    #find the article in the html page\n",
    "    jsonArticle = json.loads(soup.find(type=\"application/ld+json\").string)\n",
    "    text=jsonArticle['articleBody']\n",
    "    #remove the copyright sentence to avoid it to appear in the most frequent bigrams\n",
    "    clean_text = text.replace('.Copyright 2021 The&nbsp;Associated Press. All rights reserved. This material may not be published, broadcast, rewritten or redistributed.',' ').replace('Associated Press',' ').replace('quot',' ')\n",
    "    \n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Description: Loop on a primary list on URL to call the article_from_url function\n",
    "#Input: list of urls strings\n",
    "#Output: one string with all articles contents appended\n",
    "#Requirements : Requests, BeautifulSoup, Json\n",
    "#Use: see article_from_url\n",
    "def get_all_articles(usnews_urls):\n",
    "    all_articles = ''\n",
    "    for url in usnews_urls:\n",
    "        all_articles = all_articles +' '+article_from_url(url)\n",
    "    \n",
    "    return all_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Description: Counting the frequency of n-grams in the text\n",
    "#Input: A single string containing the text of interest \n",
    "#Output: List of bigram and their counts in the text in the format ((string,string),integer)\n",
    "#Requirement: Nltk with stopwords, Counter \n",
    "#Use: this function is set to find bigrams, it can be extended for other n-grams\n",
    "def ngram_frequency(text):\n",
    "    \n",
    "    #separate the text into words \n",
    "    allWords = nltk.tokenize.word_tokenize(text) \n",
    "    \n",
    "    #gets rid on 1-letter words and 2-letters words\n",
    "    allLongWords = []\n",
    "    for word in allWords:\n",
    "        if len(word) > 2: \n",
    "            allLongWords.append(word)   \n",
    "    #get rid of common english words\n",
    "    stopwords = nltk.corpus.stopwords.words('english') #list of words such as a, the, and etc..\n",
    "    allWordExceptStop =[]\n",
    "    for w in allLongWords:\n",
    "        if w.lower() not in stopwords:\n",
    "            allWordExceptStop.append(w)\n",
    "    #create a list of bigrams words in the text. Can be adapted to n-grams zipping more words\n",
    "    bigrams = zip(allWordExceptStop, allWordExceptStop[1:])\n",
    "    #calculate the frequency of each bigram \n",
    "    bigramsFreq = nltk.FreqDist(bigrams) \n",
    "    return bigramsFreq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Usnews.com** has a long list of [topics](https://www.usnews.com/topics/subjects). We decided to focus on political women's rights topics and we've chosen the 5 following links. We tried to run the bigram frequency with women's health and women's history included but too many words related to health or history were coming up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of primary links containing articles of interest\n",
    "URL_TOPIC_LIST = ['https://www.usnews.com/topics/subjects/feminism',\n",
    "            'https://www.usnews.com/topics/subjects/gender',\n",
    "            'https://www.usnews.com/topics/subjects/gender_bias',\n",
    "            'https://www.usnews.com/topics/subjects/sexism,\n",
    "            'https://www.usnews.com/topics/subjects/women\\'s rights' ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving of articles of interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the functions defined above we scrape the topic pages for articles references and retrieve the articles contents. The functions deal with different selection steps to avoid all the other contents at each step,i.e. advertisement, galleries, recommended articles etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_articles = ''\n",
    "for url_topic in URL_TOPIC_LIST:\n",
    "    #Retrieve all urls for latest articles in the specific feminism subject page\n",
    "    usnews_topic_urls = get_urls_usnews(url_topic)\n",
    "    \n",
    "    #Retrieve all the articles contents for the latest articles\n",
    "    all_articles_topic =  get_all_articles(usnews_topic_urls)\n",
    "    \n",
    "    #append articles to create one text\n",
    "    all_articles = all_articles +' '+all_articles_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  By MARÍA VERZA,  MEXICO CITY (AP) — Mexico’s Supreme Court ruled Tuesday that it is unconstitutional to punish abortion, unanimously annulling several provisions of a law from Coahuila — a state on the Texas border — that had made abortion a criminal act.The decision will immediately affect only the northern border state, but it establishes a historic precedent and “obligatory criteria for all of the country’s judges,” compelling them to act the same way in similar cases, said court President Arturo Zaldívar. “From now on you will not be able to, without violating the court's criteria and the constitution, charge any woman who aborts under the circumstances this court has ruled as valid.”Those circumstances will be clarified when the decision is published, but everything points to that referring to abortions carried out within the first 12 weeks of a pregnancy, the period allowed in the four states where abortion is already legal.The decision comes one week after a Texas law took effect prohibiting abortions once medical professionals can detect cardiac activity in the fetus. It allows any private citizen to sue Texas abortion providers who violate the law, as well as anyone who “aids or abets” a woman getting the procedure.Only\n"
     ]
    }
   ],
   "source": [
    "print(all_articles[0:1250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export all the articles of interest in a single text file\n",
    "text_file = open(\"generated_data/Articles-Contents.txt\", \"w\")\n",
    "text_file.write(all_articles)\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency computation for bigrams "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Couting bigram frequencies for all articles of interest\n",
    "usNewsFEMbigramFreq = ngram_frequency(all_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Los', 'Angeles');23\n",
      "('gender', 'equality');22\n",
      "('New', 'York');20\n",
      "('Black', 'women');13\n",
      "('child', 'care');12\n",
      "('Angeles', 'County');12\n",
      "('health', 'care');11\n",
      "('men', 'pay');11\n",
      "('percentage', 'men');11\n",
      "('White', 'House');11\n",
      "('Women', 'pay');10\n",
      "('pay', 'percentage');10\n",
      "('United', 'States');9\n",
      "('Best', 'Countries');9\n",
      "('Hillary', 'Clinton');8\n",
      "('vice', 'president');8\n",
      "('sexual', 'harassment');8\n",
      "('women', 'girls');7\n",
      "('Washington', 'D.C.');7\n",
      "('girls', 'women');7\n",
      "('electoral', 'system');7\n",
      "('Donald', 'Trump');7\n",
      "('Soul', 'Woman');6\n",
      "('first', 'time');6\n",
      "('Supreme', 'Court');6\n",
      "('coronavirus', 'pandemic');6\n",
      "('Countries', 'rankings');6\n",
      "('Middle', 'East');6\n",
      "('share', 'women');6\n",
      "('one', 'highest');6\n",
      "('women', 'according');6\n",
      "('Rhode', 'Island');6\n",
      "('rates', 'women');6\n",
      "('North', 'Carolina');6\n",
      "('Board', 'Supervisors');6\n",
      "('female', 'mayors');6\n",
      "('social', 'media');5\n",
      "('gender', 'stereotypes');5\n",
      "('five', 'years');5\n",
      "('Ford', 'Foundation');5\n",
      "('Hayes', 'said');5\n",
      "('women', 'movement');5\n",
      "('Mexico', 'City');5\n",
      "('see', 'women');5\n",
      "('good', 'job');5\n",
      "('public', 'schools');5\n",
      "('state', 'budgets');5\n",
      "('death', 'rate');5\n",
      "('U.S.', 'News');5\n",
      "('Asia', 'Middle');5\n"
     ]
    }
   ],
   "source": [
    "MAX = 50\n",
    "\n",
    "#Visualize the most common bigrams\n",
    "for word, frequency in usNewsFEMbigramFreq.most_common(MAX):\n",
    "        print('%s;%d' % (word, frequency))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common bigrams list also contain a lot of Named Entities (NE) like cities, persons etc... \n",
    "We can see *'Los', 'Angeles'* and *'Donald' 'Trump'* as common bigrams.\n",
    "Here we use the naive approach to ignore this name by using the word capitalization to select them. Note that there are more advanced way to recognize NE (for example Stanforde NER library) but we believe that it will be overkilled for our usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gender equality',\n",
       " 'child care',\n",
       " 'health care',\n",
       " 'men pay',\n",
       " 'percentage men',\n",
       " 'pay percentage',\n",
       " 'vice president',\n",
       " 'sexual harassment',\n",
       " 'women girls',\n",
       " 'girls women',\n",
       " 'electoral system',\n",
       " 'first time',\n",
       " 'coronavirus pandemic',\n",
       " 'share women',\n",
       " 'one highest',\n",
       " 'women according',\n",
       " 'rates women',\n",
       " 'female mayors',\n",
       " 'social media',\n",
       " 'gender stereotypes',\n",
       " 'five years',\n",
       " 'women movement',\n",
       " 'see women',\n",
       " 'good job',\n",
       " 'public schools',\n",
       " 'state budgets',\n",
       " 'death rate',\n",
       " 'gender gap',\n",
       " 'sex discrimination',\n",
       " 'states women',\n",
       " 'top states',\n",
       " 'women representation',\n",
       " 'became first',\n",
       " 'lose weight',\n",
       " 'six years',\n",
       " 'women rights',\n",
       " 'first nonfiction',\n",
       " 'nonfiction book',\n",
       " 'book decade',\n",
       " 'year pandemic',\n",
       " 'really began',\n",
       " 'woman time',\n",
       " '100 million',\n",
       " 'public school',\n",
       " 'federal government',\n",
       " 'based gender',\n",
       " 'regions say',\n",
       " 'young people',\n",
       " 'women still',\n",
       " 'gender-based violence',\n",
       " 'best states',\n",
       " 'top five',\n",
       " 'states plus',\n",
       " 'top economic',\n",
       " 'education health']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_final_list = []\n",
    "MAX = 100\n",
    "for word, frequency in usNewsFEMbigramFreq.most_common(MAX):\n",
    "    if (word[0][0].isupper()==False and word[1][0].isupper()==False): #ignore the Named Entities\n",
    "        bigram_final_list.append(word[0]+' '+word[1])\n",
    "\n",
    "bigram_final_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use this list (in its 150 word long vesrion) to extend our personal list of bigrams. However, maybe because of the corpus size there are still some bigrams that aren't of interest. For example, the *health care* or *vice president* are ignored because the former is too general and the latter irrelevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"women's right\",\n",
       " 'Equal opportunities',\n",
       " 'Equal rights',\n",
       " 'Equal status',\n",
       " 'equal pay',\n",
       " 'gender gap',\n",
       " 'Gender discrimination',\n",
       " 'Gender equality',\n",
       " 'Sexual harrasment',\n",
       " 'Women empowerment',\n",
       " 'women victim',\n",
       " 'women immigration',\n",
       " 'Women emancipation',\n",
       " \"women's participation\",\n",
       " 'Western women',\n",
       " 'non-western woman',\n",
       " 'Muslim women',\n",
       " 'Equal wages',\n",
       " 'Gender equality',\n",
       " 'gender equity',\n",
       " 'Men and women',\n",
       " 'women and men',\n",
       " 'women oppression',\n",
       " 'niqab banstruggle of girls',\n",
       " 'struggle of women',\n",
       " 'war against women',\n",
       " 'oppression of girls',\n",
       " 'oppression of women',\n",
       " 'women oppression',\n",
       " \"women's opression\",\n",
       " 'liberate women',\n",
       " 'religious oppresion',\n",
       " 'abuse of women',\n",
       " 'Male oppression',\n",
       " 'Female oppression',\n",
       " 'Exploitation of women',\n",
       " 'Indigenous women',\n",
       " 'Patriarchal culture',\n",
       " 'gender equality',\n",
       " 'child care',\n",
       " 'men pay',\n",
       " 'percentage men',\n",
       " 'pay percentage',\n",
       " 'sexual harassment',\n",
       " 'women girls',\n",
       " 'girls women',\n",
       " 'rates women',\n",
       " 'women according',\n",
       " 'female mayors',\n",
       " 'share women',\n",
       " 'women movement',\n",
       " 'see women',\n",
       " 'gender stereotypes',\n",
       " 'gender gap',\n",
       " 'women representation',\n",
       " 'sex discrimination',\n",
       " 'states women',\n",
       " 'lose weight',\n",
       " 'women rights',\n",
       " 'woman time',\n",
       " 'based gender',\n",
       " 'proportional electoral',\n",
       " 'female candidates',\n",
       " 'gender-based violence',\n",
       " 'entirely female',\n",
       " 'cities female']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_usnews_keywords = ['gender equality','child care','men pay','percentage men',\n",
    "              'pay percentage','sexual harassment','women girls','girls women',\n",
    "              'rates women','women according','female mayors','share women','women movement',\n",
    "              'see women','gender stereotypes','gender gap',\n",
    "              'women representation','sex discrimination','states women','lose weight',\n",
    "              'women rights','woman time',\n",
    "              'based gender',\n",
    "              'proportional electoral','female candidates','gender-based violence','entirely female','cities female']\n",
    "            \n",
    "#Personal keywords list \n",
    "my_bigrams = ['women\\'s right','Equal opportunities','Equal rights','Equal status',\n",
    "           'equal pay','gender gap','Gender discrimination','Gender equality','Sexual harrasment','Women empowerment',\n",
    "            'women victim','women immigration','Women emancipation','women\\'s participation','Western women','non-western woman',\n",
    "              'Muslim women', 'Equal wages','Gender equality',\n",
    "             'gender equity','Men and women', 'women and men', 'women oppression', 'niqab ban'\n",
    "           'struggle of girls','struggle of women', 'war against women','oppression of girls','oppression of women',\n",
    "           'women oppression','women\\'s opression','liberate women','religious oppresion',\n",
    "           'abuse of women','Male oppression','Female oppression','Exploitation of women',\n",
    "           'Indigenous women','Patriarchal culture']\n",
    "\n",
    "all_bigrams = my_bigrams + selected_usnews_keywords\n",
    "all_bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step save a keywords text file and recall it. This is done once to save important information but the notebook could ba run directly without the export and import."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export all the keywords in a single text file\n",
    "text_file = open(\"generated_data/Keywords.txt\", \"w\")\n",
    "for bigram in all_bigrams:\n",
    "    text_file.write(bigram+',')\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"women's right\",\n",
       " 'Equal opportunities',\n",
       " 'Equal rights',\n",
       " 'Equal status',\n",
       " 'equal pay',\n",
       " 'gender gap',\n",
       " 'Gender discrimination',\n",
       " 'Gender equality',\n",
       " 'Sexual harrasment',\n",
       " 'Women empowerment',\n",
       " 'women victim',\n",
       " 'women immigration',\n",
       " 'Women emancipation',\n",
       " \"women's participation\",\n",
       " 'Western women',\n",
       " 'non-western woman',\n",
       " 'Muslim women',\n",
       " 'Equal wages',\n",
       " 'Gender equality',\n",
       " 'gender equity',\n",
       " 'Men and women',\n",
       " 'women and men',\n",
       " 'women oppression',\n",
       " 'niqab banstruggle of girls',\n",
       " 'struggle of women',\n",
       " 'war against women',\n",
       " 'oppression of girls',\n",
       " 'oppression of women',\n",
       " 'women oppression',\n",
       " \"women's opression\",\n",
       " 'liberate women',\n",
       " 'religious oppresion',\n",
       " 'abuse of women',\n",
       " 'Male oppression',\n",
       " 'Female oppression',\n",
       " 'Exploitation of women',\n",
       " 'Indigenous women',\n",
       " 'Patriarchal culture',\n",
       " 'gender equality',\n",
       " 'child care',\n",
       " 'men pay',\n",
       " 'percentage men',\n",
       " 'pay percentage',\n",
       " 'sexual harassment',\n",
       " 'women girls',\n",
       " 'girls women',\n",
       " 'rates women',\n",
       " 'women according',\n",
       " 'female mayors',\n",
       " 'share women',\n",
       " 'women movement',\n",
       " 'see women',\n",
       " 'gender stereotypes',\n",
       " 'gender gap',\n",
       " 'women representation',\n",
       " 'sex discrimination',\n",
       " 'states women',\n",
       " 'lose weight',\n",
       " 'women rights',\n",
       " 'woman time',\n",
       " 'based gender',\n",
       " 'proportional electoral',\n",
       " 'female candidates',\n",
       " 'gender-based violence',\n",
       " 'entirely female',\n",
       " 'cities female',\n",
       " '']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import all the keywords in a single text file\n",
    "KEYWORDS_LIST = [] \n",
    "# opening the text file\n",
    "with open(\"generated_data/Keywords.txt\", \"r\") as file:\n",
    " \n",
    "    # reading each line    \n",
    "    for line in file:\n",
    "   \n",
    "        # reading each word        \n",
    "        for word in line.split(','):\n",
    "   \n",
    "            # displaying the words           \n",
    "            KEYWORDS_LIST.append(word) \n",
    "KEYWORDS_LIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset selection from Quotebank database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processing on chunk\n",
    "#Input\n",
    "#Output\n",
    "def process_chunk(chunk, vocabulary):\n",
    "    print(f'Processing chunk with {len(chunk)} rows')\n",
    "    #print(chunk.columns)\n",
    "    occurences = np.zeros(len(vocabulary))\n",
    "    for index, word in enumerate(vocabulary):\n",
    "        occurences[index] = np.sum(chunk['quotation'].str.contains(word)) \n",
    "    return occurences\n",
    "\n",
    "#Select quotes containing keywords\n",
    "def select_quotes_chunk(chunk, keywords):\n",
    "    print(f'Processing chunk with {len(chunk)} rows')\n",
    "    return chunk[chunk['quotation'].str.contains('|'.join(keywords))]\n",
    "\n",
    "#Use the selection function on each chunk of the full dataset \n",
    "def select_quotes_one_year(path_to_file, vocabulary, chunksize = 10 ** 4):\n",
    "    with pd.read_json(path_to_file, lines=True, compression='bz2', chunksize=chunksize) as df_reader:\n",
    "        for index, chunk in enumerate(df_reader):\n",
    "            if not index==0:\n",
    "                selected_df = pd.concat([selected_df, select_quotes_chunk(chunk, vocabulary)])\n",
    "            else: \n",
    "                selected_df = select_quotes_chunk(chunk, vocabulary)\n",
    "    return selected_df\n",
    "\n",
    "#Use the selection function on each chunk of the full dataset \n",
    "#Dumps the selected quotes into a new json file\n",
    "def select_and_dump(path_to_file, vocabulary, chunksize = 10 ** 4, year = 'replace_me'):\n",
    "    with pd.read_json(path_to_file, lines=True, compression='bz2', chunksize=chunksize) as df_reader:\n",
    "        for index, chunk in enumerate(df_reader):\n",
    "            #Dump selected quotes\n",
    "            selected_df = select_quotes_chunk(chunk, vocabulary)\n",
    "            pickle_file_name = year + '_chunk_' + str(index) + '.pkl'\n",
    "            selected_df.to_pickle('files/'+pickle_file_name)\n",
    "            #if not index==0:\n",
    "                #selected_df = pd.concat([selected_df, select_quotes_chunk(chunk, vocabulary)])\n",
    "            #else: \n",
    "               # selected_df = select_quotes_chunk(chunk, vocabulary)\n",
    "    return selected_df\n",
    "\n",
    "\n",
    "import random, string\n",
    "\n",
    "def randomword(length):\n",
    "    letters = string.ascii_lowercase\n",
    "    return ''.join(random.choice(letters) for i in range(length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add the import of keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = 'data/'\n",
    "QUOTEBANK_2020 = DATA_FOLDER+ \"quotes-2020.json.bz2\"\n",
    "QUOTEBANK_2019 = DATA_FOLDER+ \"quotes-2019.json.bz2\"\n",
    "QUOTEBANK_2017 = DATA_FOLDER+ \"quotes-2017.json.bz2\"\n",
    "QUOTEBANK_2015 = DATA_FOLDER+ \"quotes-2015.json.bz2\"\n",
    "QUOTEBANK_2018 = DATA_FOLDER+ \"quotes-2018.json.bz2\"\n",
    "QUOTEBANK_2016 = DATA_FOLDER+ \"quotes-2016.json.bz2\"\n",
    "\n",
    "\n",
    "KEYWORDS_LIST = ('women\\'s right','Equal opportunities','Equal rights','Equal status','equal pay',\n",
    "              'gender gap','Gender discrimination','Gender equality','Sexual harrassment',\n",
    "              'Women empowerment','women victim','women immigration','Women emancipation',\n",
    "              'women\\'s participation','Western women','non-western woman','Muslim women',\n",
    "              'Equal wages','Gender equality','gender equity','Men and women','women and men',\n",
    "              'women oppression','niqab ban','struggle of girls','struggle of women','war against women',\n",
    "              'oppression of girls','oppression of women','women oppression','women\\'s opression','liberate women',\n",
    "              'religious oppresion','abuse of women','Male oppression','Female oppression','Exploitation of women',\n",
    "              'Indigenous women','Patriarchal culture','gender equality','child care','men pay','percentage men',\n",
    "              'pay percentage','sexual harassment','women girls','girls women',\n",
    "              'rates women','women according','female mayors','share women','women movement',\n",
    "              'see women','gender stereotypes','gender gap',\n",
    "              'women representation','sex discrimination','states women','lose weight',\n",
    "              'women rights','woman time',\n",
    "              'based gender',\n",
    "              'proportional electoral','female candidates','gender-based violence','entirely female','cities female')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEYWORDS_LIST = f.read(keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select and pickle of quotes of interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: This code has to be run once to create the pickle files containing the quotes of interest. For futher use, the dataframe is direcly loaded from the pickle files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %time QOI_2015_DF = select_quotes_one_year(QUOTEBANK_2015,KEYWORDS_LIST,10 ** 4)\n",
    "# %time QOI_2016_DF = select_quotes_one_year(QUOTEBANK_2016,KEYWORDS_LIST,10 ** 4)\n",
    "# %time QOI_2017_DF = select_quotes_one_year(QUOTEBANK_2017,KEYWORDS_LIST,10 ** 4)\n",
    "# %time QOI_2018_DF = select_quotes_one_year(QUOTEBANK_2018,KEYWORDS_LIST,10 ** 4)\n",
    "# %time QOI_2019_DF = select_quotes_one_year(QUOTEBANK_2019,KEYWORDS_LIST,10 ** 4)\n",
    "# %time QOI_2020_DF = select_quotes_one_year(QUOTEBANK_2020,KEYWORDS_LIST,10 ** 4)\n",
    "\n",
    "dataframesNames = ('QOI_2015_DF','QOI_2016_DF','QOI_2017_DF','QOI_2018_DF','QOI_2019_DF','QOI_2020_DF')\n",
    "#dataframes = (QOI_2015_DF,QOI_2016_DF,QOI_2017_DF,QOI_2018_DF,QOI_2019_DF,QOI_2020_DF)\n",
    "\n",
    "#for i in range(len(dataframesNames)):\n",
    "#    dataframes[i].to_pickle('generated_data/'+dataframesNames[i]+'.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataframe of selected quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'generated_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate into one dataframes the dataframes from each pickle file. \n",
    "df = pd.concat([pd.read_pickle(PATH+ fp +'.pkl') for fp in dataframesNames], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quoteID</th>\n",
       "      <th>quotation</th>\n",
       "      <th>speaker</th>\n",
       "      <th>qids</th>\n",
       "      <th>date</th>\n",
       "      <th>numOccurrences</th>\n",
       "      <th>probas</th>\n",
       "      <th>urls</th>\n",
       "      <th>phase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-03-09-004706</td>\n",
       "      <td>Anything less than women winning 50 per cent o...</td>\n",
       "      <td>Katy Gallagher</td>\n",
       "      <td>[Q463507]</td>\n",
       "      <td>2015-03-09 12:30:00</td>\n",
       "      <td>1</td>\n",
       "      <td>[[Katy Gallagher, 0.5872], [None, 0.4128]]</td>\n",
       "      <td>[http://www.smh.com.au/act-news/women-need-to-...</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-04-24-025718</td>\n",
       "      <td>I'd like to congratulate all the winners and f...</td>\n",
       "      <td>Helena Morrissey</td>\n",
       "      <td>[Q23762081]</td>\n",
       "      <td>2015-04-24 15:33:00</td>\n",
       "      <td>1</td>\n",
       "      <td>[[Helena Morrissey, 0.8706], [None, 0.1294]]</td>\n",
       "      <td>[http://www.cipd.co.uk/PM/peoplemanagement/b/w...</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-07-16-044620</td>\n",
       "      <td>I think what Deepika has spoken in the video m...</td>\n",
       "      <td>Kalki Koechlin</td>\n",
       "      <td>[Q3192216]</td>\n",
       "      <td>2015-07-16 16:41:07</td>\n",
       "      <td>1</td>\n",
       "      <td>[[Kalki Koechlin, 0.6377], [None, 0.3623]]</td>\n",
       "      <td>[http://www.pinkvilla.com/entertainmenttags/ka...</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-09-11-052815</td>\n",
       "      <td>if advocating for equal pay for equal work is ...</td>\n",
       "      <td>Hillary Clinton</td>\n",
       "      <td>[Q6294]</td>\n",
       "      <td>2015-09-11 14:17:08</td>\n",
       "      <td>1</td>\n",
       "      <td>[[Hillary Clinton, 0.8831], [None, 0.1105], [D...</td>\n",
       "      <td>[http://www.wrn.com/2015/09/hillary-clinton-ra...</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-04-23-037713</td>\n",
       "      <td>Men and women are understandably upset if they...</td>\n",
       "      <td>Jim McDermott</td>\n",
       "      <td>[Q321457, Q6196778]</td>\n",
       "      <td>2015-04-23 21:52:22</td>\n",
       "      <td>1</td>\n",
       "      <td>[[Jim McDermott, 0.629], [John F. Kerry, 0.190...</td>\n",
       "      <td>[http://www.atlanticcouncil.org/en/blogs/new-a...</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             quoteID                                          quotation  \\\n",
       "0  2015-03-09-004706  Anything less than women winning 50 per cent o...   \n",
       "1  2015-04-24-025718  I'd like to congratulate all the winners and f...   \n",
       "2  2015-07-16-044620  I think what Deepika has spoken in the video m...   \n",
       "3  2015-09-11-052815  if advocating for equal pay for equal work is ...   \n",
       "4  2015-04-23-037713  Men and women are understandably upset if they...   \n",
       "\n",
       "            speaker                 qids                date  numOccurrences  \\\n",
       "0    Katy Gallagher            [Q463507] 2015-03-09 12:30:00               1   \n",
       "1  Helena Morrissey          [Q23762081] 2015-04-24 15:33:00               1   \n",
       "2    Kalki Koechlin           [Q3192216] 2015-07-16 16:41:07               1   \n",
       "3   Hillary Clinton              [Q6294] 2015-09-11 14:17:08               1   \n",
       "4     Jim McDermott  [Q321457, Q6196778] 2015-04-23 21:52:22               1   \n",
       "\n",
       "                                              probas  \\\n",
       "0         [[Katy Gallagher, 0.5872], [None, 0.4128]]   \n",
       "1       [[Helena Morrissey, 0.8706], [None, 0.1294]]   \n",
       "2         [[Kalki Koechlin, 0.6377], [None, 0.3623]]   \n",
       "3  [[Hillary Clinton, 0.8831], [None, 0.1105], [D...   \n",
       "4  [[Jim McDermott, 0.629], [John F. Kerry, 0.190...   \n",
       "\n",
       "                                                urls phase  \n",
       "0  [http://www.smh.com.au/act-news/women-need-to-...     E  \n",
       "1  [http://www.cipd.co.uk/PM/peoplemanagement/b/w...     E  \n",
       "2  [http://www.pinkvilla.com/entertainmenttags/ka...     E  \n",
       "3  [http://www.wrn.com/2015/09/hillary-clinton-ra...     E  \n",
       "4  [http://www.atlanticcouncil.org/en/blogs/new-a...     E  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataframe has 87161 entries\n"
     ]
    }
   ],
   "source": [
    "print(f'The dataframe has {len(df)} entries')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Anything less than women winning 50 per cent of new seats will be a loss not only for a progressive city's progress towards true gender equality but it would also be a loss for good governance in ...\n",
       "1    I'd like to congratulate all the winners and finalists on their success. They have demonstrated clear leadership by moving women's progression from a `diversity' initiative to a core business prio...\n",
       "2    I think what Deepika has spoken in the video makes sense. I do understand the counter argument too where everyone has been saying that had men said the same lines about having sex outside marriage...\n",
       "3                                                                                    if advocating for equal pay for equal work is playing the gender card, deal me in. I am ready to play as hard as I can.\n",
       "4      Men and women are understandably upset if they see a company close down and jobs lost. It's only natural people would look around and in their distress they find something or someone able to blame,\n",
       "Name: quotation, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_colwidth = 200\n",
    "df.head()['quotation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
